#!/usr/bin/env python

desc = """

MongoDB sharded cluster backups.

"""

from bongo.mongo import BongoMongo
from bongo.zk import BongoZk
from bongo.aws import BongoAws
import yaml
import logging
from logging.handlers import SysLogHandler
import sys
import os
import traceback
import argparse
from time import sleep
from datetime import datetime
from random import randint


CMDMODE = 'BongoCommandMode'


class BackupLogFilter(logging.Filter):
   def __init__(self, *allowed):
      self.allowed = [logging.Filter(name) for name in allowed]

   def filter(self, record):
      return any(f.filter(record) for f in self.allowed)


class BackupError(Exception):
   pass


class BackupsDisabled(Exception):
   pass


class BackupAbort(Exception):
   def __init__(self, bz, msg):
      try:
         bz.abort(msg)
      except:
         pass
      logging.critical(msg)
      raise SystemExit(1)


class BackupServerType:
   Mongos = 'mongodb_mongos_server'
   Config = 'mongodb_config_server'
   Shard = 'mongodb_shard_server'

server_type = BackupServerType()


class BackupEvents:
   BackupsComplete = server_type.Mongos + '/backups_complete'
   BalancerStopped = server_type.Mongos + '/balancer_stopped'
   BalancerStarted = server_type.Mongos + '/balancer_started'
   ConfigServerBackup = server_type.Config + '/configdb_backup'
   ConfigServerShutdown = server_type.Config + '/shutdown'
   ConfigServerStarted = server_type.Config + '/started'

event = BackupEvents()


class ConfigKeys:
   Clean = 'clean'

configKeys = ConfigKeys


def positive_integer(i):
   try:
      i = int(i)
      if i < 1: raise ValueError
   except ValueError:
      raise argparse.ArgumentTypeError("argument must be a "
                                       "positive integer")
   return i


def non_negative_integer(i):
   try:
      i = int(i)
      if i < 0: raise ValueError
   except ValueError:
      raise argparse.ArgumentTypeError("argument must be a "
                                       "non-negative integer")
   return i


def parse_args():

   gargle = argparse.ArgumentParser(prog = "bongo", description=desc,
	       usage='%(prog)s [options] <value>',
	       formatter_class = argparse.RawDescriptionHelpFormatter)

   gen_opts = gargle.add_argument_group(title="General Arguments")

   gen_opts.add_argument('--zkconfig', dest='zkconfig', metavar="<file_path>",
		      help="""zookeeper server list file
                           (default: /etc/zookeeper/server_list.yml)""",
		      default='/etc/zookeeper/server_list.yml')

   gen_opts.add_argument('--zkroot', dest='zkroot', metavar="<string>",
		      help="""zookeeper root path - The cluster arg will be
                           appended to this. (default: /backup/mongodb_cluster)""",
		      default='/backup/mongodb_cluster')

   gen_opts.add_argument('--cluster', dest='env', metavar="<string>",
		      help="""MongoDB cluster name, e.g. production, qa, demo,
                           etc. (default: development)""",
		      default='development')

   gen_opts.add_argument('--verbose', action='store_true',
		      help='log to stderr in addition to syslog (default: False)')

   bkup_opts = gargle.add_argument_group(title="Backup Arguments")

   bkup_opts.add_argument('--data', dest='data', metavar="<string>",
		      help='the DB data volume path (default: /data)',
		      default='/data')

   bkup_opts.add_argument('--s3bucket', dest='s3bucket', metavar="<string>",
		      help="""S3 bucket in which the config database backups
                           are stored (default: ops.tlium.com)""",
		      default='ops.tlium.com')

   bkup_opts.add_argument('--s3key', dest='s3key', metavar="<string>",
                      help="""S3 key prefix used to store the config database
                           backups - The actual key used is
                           <s3key>/<cluster>/config_server/<date>-<time> 
                           (default: backups/mongodb)""",
		      default='backups/mongodb')

   bkup_opts.add_argument('--solo', action='store_true',
		         help="""Only snapshot the local DB array devices
                              (no config DB backup, no coordination with other
                              DB servers, no ZK interaction, etc.).  Does not
                              care if the shard balancer is running and will happily
                              lock your primary against writes during the
                              snapshots, so... good luck with that.""")

   bkup_opts.add_argument('--unlock', action='store_true',
			help="""Debugging utility:  If the local database is a locked secondary, performs an fsyncUnlock().""")

   cfg_opts = gargle.add_argument_group("Global Configuration Arguments")

   ex_grp = cfg_opts.add_mutually_exclusive_group()

   ex_grp.add_argument('--enable', action='store_true',
                         help='enable cluster backups (per environment)')

   ex_grp.add_argument('--disable', dest='disable', metavar="<reason>",
         help='disable cluster backups (per environment) (default: None)')

   ex_grp.add_argument('--clean', dest='clean', metavar="<int>",
                       type=non_negative_integer,
                       help="""configure to discard backups older than the given number of days
                            along with the corresponding ZK entries (per environment)
                            (default: None)""")

   #ex_grp.add_argument('--source', dest='source', metavar="<region>",
   #      help='set backup source region for cluster (per environment) (default: us-west-1)')

   #ex_grp.add_argument('--copy', action='store_true',
   #                   help='copy snapshots to current region (default: False)')

   return gargle.parse_args()


def whoami():
   from socket import gethostbyname
   from socket import getfqdn
   try:
      fqdn = getfqdn()
   except Exception as e:
      logging.warning("can't get my FQDN: %s" % e)
   try:
      ip = ""
      if fqdn:
         ip = gethostbyname(fqdn)
   except Exception as e:
      logging.warning("can't get my IP: %s" % e)
   if fqdn and ip:
      whoami = "%s (%s)" % (fqdn, ip)
   elif fqdn:
      whoami = fqdn
   elif ip:
      whoami = ip
   else:
      whoami = None
      logging.warning("I don't know who I am!")
   return whoami


def get_local_mnt_dev(path):

   import subprocess

   cmd = "/bin/df | /bin/grep '%s' | /usr/bin/awk '{print $1}'" % path

   dev = subprocess.check_output(cmd, shell=True).rstrip('\n')

   return dev


def get_local_uuid(path):

   import subprocess

   dev = get_local_mnt_dev(path)

   cmd = "/sbin/mdadm -D %s | /bin/grep UUID | /usr/bin/awk -F' : ' '{print $2}'" \
         % dev
   
   uuid = subprocess.check_output(cmd, shell=True).rstrip('\n')

   return uuid


def get_local_raid_devs(path):

   import subprocess

   data_md = get_local_mnt_dev(path)

   cmd = "/sbin/mdadm -D %s | /bin/grep active | /usr/bin/awk '{print $7}'" \
         % data_md

   devs = subprocess.check_output(cmd, shell=True)

   return [d.replace('xvd','sd') for d in devs.split('\n')]


def snapshot_ebs_raid(aws, vol_path, snapshot_name, snapshot_description, cluster='', source=''):

   try:

      array_uuid = get_local_uuid(vol_path)

      local_devs = get_local_raid_devs(vol_path)

      vols = aws.get_ebs_vols(local_devs)

      snaps = []

      # Snap each volume id in ebs_vols
      for vol in vols:

         desc = "%s - %s" % (snapshot_description, vol['dev'])

         try:

            snap_id = aws.create_snapshot(vol['id'], desc,
                                          Name=snapshot_name,
                                          UUID=array_uuid,
                                          Cluster=cluster,
                                          Source=source)
            snaps.append(snap_id)

         except BongoAws.BongoAwsException as e:
            raise BackupError("Error creating snapshot of EBS volume %s: %s"
                              % (ebs, e))
      
      return snaps

   except BongoAws.BongoAwsException as e:
      raise BackupError("Error creating EBS snapshots: %s" % e)


def disable_backups(zk_root, zk_servers, reason):
   """Use BongoZk to set disabled flag."""

   try:

      bz = BongoZk(zk_root,
                   CMDMODE,
                   servers = zk_servers)

      bz.disable(reason)
      bz.stop()

   except BongoZk.BongoZkServerTypeExists:
      logging.info("Error: apparently someone else is using cmd mode right now.")
      raise SystemExit(1) 
   except BongoZk.BongoZkException as e:
      raise BackupAbort(bz,
                        "disable_backups: aborting with "
                        "ZooKeeper-related error: %s" % e)


def enable_backups(zk_root, zk_servers):
   """Use BongoZk to unset disabled flag."""

   try:

      bz = BongoZk(zk_root,
                   CMDMODE,
                   servers = zk_servers)

      bz.enable()
      bz.stop()

   except BongoZk.BongoZkServerTypeExists:
      logging.info("Error: apparently someone else is using cmd mode right now.")
      raise SystemExit(1) 
   except BongoZk.BongoZkException as e:
      raise BackupAbort(bz,
                        "enable_backups: aborting with "
                        "ZooKeeper-related error: %s" % e)


def config_set(zk_root, zk_servers, key, value):
   """Use BongoZk to store config values."""

   try:

      bz = BongoZk(zk_root,
                   CMDMODE,
                   servers = zk_servers)

      bz.config_set(key, value)
      bz.stop()

   except BongoZk.BongoZkServerTypeExists:
      logging.info("Error: apparently someone else is using cmd mode right now.")
      raise SystemExit(1) 
   except BongoZk.BongoZkException as e:
      raise BackupAbort(bz,
                        "disable_backups: aborting with "
                        "ZooKeeper-related error: %s" % e)

      
def unlock_controller(db):

   try:
      if not db.is_primary:
	 db.unlock()

   except BongoMongo.BongoMongoException as e:
      raise SystemExit("unlock_controller: aborting with MongoDB-related error: %s"
                       % e)
   except Exception as e:
      traceback.print_exc(file=sys.stderr)
      raise SystemExit("unlock_controller: aborting with uncaught exception: %s" % e)


def solo_controller(db, data, env):
   """
   - lock my db
   - snapshot my raid vols
   - unlock my db
   """

   try:

      replset_name = db.replica_set_name()

      if not replset_name:
         raise Exception("cannot determine replica set name")

      snap_desc = "%s - %s" % (env, replset_name)
      snap_name = "DB Backup - %s" % replset_name

      id = whoami()

      aws = BongoAws()

      logging.info("solo_controller: locking my db against writes")
      db.lock()

      logging.info("solo_controller: creating snapshots of my raid vols")
      snapshots = snapshot_ebs_raid(aws, data, snap_name, snap_desc, env, id)

      logging.info("solo_controller: unlocking my db")
      db.unlock()

      logging.info("solo_controller: created snapshots: %s" % snapshots)
      logging.info("solo_controller: done")

   except BackupsDisabled:
      raise
   except BongoMongo.BongoMongoException as e:
      db.unlock()
      raise SystemExit("solo_controller: aborting with MongoDB-related error: %s"
                       % e)
   except BongoAws.BongoAwsException as e:
      db.unlock()
      raise SystemExit("solo_controller: aborting with AWS-related error: %s" % e)
   except BackupError as e:
      db.unlock()
      raise SystemExit("solo_controller: aborting: %s" % e)
   except Exception as e:
      db.unlock()
      traceback.print_exc(file=sys.stderr)
      raise SystemExit("solo_controller: aborting with uncaught exception: %s" % e)


def mongos_controller(db, zk_root, zk_servers):
   """
   - get my lock & join the party (done by BongoZk)
   - get list of shards... wait until party size >= shard count + 
     1 for config server + 1 for myself
   - stop the balancer (if needed)
   - wait for config server backup to complete
   - wait until shard backups are finished
   - wait for a config server to startup
   - wait until everyone else leaves the party (party size = 1)
   - confirm that I'm still at the party
   - start the balancer (if needed)
   - leave the party and unlock (done by BongoZk)
   """

   ignore_balancer = False

   def my_abort(msg=None):
      logging.critical("mongos_controller: abort event detected: %s" % msg)

      if ignore_balancer:
         logging.critical("mongos_controller: ignoring balancer status")
      else:
         logging.critical("mongos_controller: attempting balancer restart")
         db.start_balancer()

      logging.critical("mongos_controller: exiting...")
      os._exit(1)

   try:

      bz = BongoZk(zk_root,
                   server_type.Mongos,
                   servers = zk_servers, 
                   hour=True)

      if not bz.is_enabled():
         raise BackupsDisabled

      bz.watch4abort(my_abort)

      aws = BongoAws()

      # Record our ID and region info
      me = {'server': bz.id, 'region' : aws.region}
      bz.set_znode4today(str(me))

      # 1 server (min) per shard + 1 config server + 1 mongos server = how big
      # the party should get
      party_capacity = len(db.shards) + 2 

      # can only wait so long for the party to rock... check every 10 secs
      # for 10 minutes (60 loops * 10 sec sleep between loops = 600 secs = 10 mins).
      patience = 60

      logging.info("mongos_controller: waiting for the party to rock (expected size = %d)" % party_capacity)
      while bz.party_size() < party_capacity:
	 logging.info("mongos_controller: current party size = %d" % bz.party_size())
         if patience:
            patience -= 1
         else:
            raise BackupError("took too long (10+ mins) for all the backup "
                              "servers to get started")
         sleep(10)

      # if we get here, we can assume everybody showed up
      logging.info("mongos_controller: party is rocking (size = %d)"
                   % bz.party_size())

      # cluster balancer fun:  if the balancer is already stopped, we want
      # to leave it that way - it might be off for a good reason!

      if db.balancer_enabled():
         ignore_balancer = False
         logging.info("mongos_controller: stopping balancer")
         db.stop_balancer()
      else:
         ignore_balancer = True

      bz.notify(event.BalancerStopped)

      # Strictly speaking this bit is incorrect.  It waits for ".../mongodb_shard_server/<SHARD_NAME>" but the
      # the shard servers write their completion info to ".../mongodb_shard_server/<SHARD_NAME>/<REGION>".  For
      # multi-region replica sets there will two or more of those node paths, but we can't wait on any region
      # terminated paths because we do not know in which regions the shard servers are located.  We could probably
      # figure it out from here (and bury the nasty details in bongo.mongo) but it may not be worth it.
      # It would be nice to find a more elegant way."

      # Actually I seriously just thought of an elegant way to solve it, but there is not enough room in the margins
      # of this edit buffer to contain the proof.

      for shard in db.shards:
         logging.info("mongos_controller: waiting for %s backup to complete"
                      % shard)
	 try:
	    shard_event = server_type.Shard + "/" + shard
	    bz.wait4event(event=shard_event, timeout=600)
	 except BongoZk.BongoZkEventTimeout as e:
	    raise BackupAbort(bz,
		  "mongos_controller: %s: timed out waiting for %s to complete backup" % (bz.id, shard))

      # config db and all the shards are backed up at this point, in theory

      bz.notify(event.BackupsComplete)

      logging.info("mongos_controller: waiting for config server db startup")
      try:
	 bz.wait4event(event=event.ConfigServerStarted, timeout=300)
      except BongoZk.BongoZkEventTimeout as e:
	 raise BackupAbort(bz,
	       "mongos_controller: %s: timed out waiting for config server to restart" % bz.id)

      patience = 360

      logging.info("mongos_controller: waiting for the party to end")
      while bz.party_size() > 1:
         if patience:
            patience = patience - 1
         else:
            raise BackupError("took too long (60+ mins) for all the backup "
                              "servers to leave the party")
         sleep(10)

      if not ignore_balancer:
         logging.info("mongos_controller: starting balancer")
         db.start_balancer()
      else:
         logging.warning("mongos_controller: leaving cluster balancer "
                     "disabled (as I found it)")

      bz.notify(event.BalancerStarted)

      clean_days = int(bz.config_get(configKeys.Clean))

      if clean_days:

         logging.info("mongos_controller: cleaning backup info older than %d days..." % clean_days)

         for old_backup in bz.archive(clean_days, server_type.Mongos):

            bz.delete(old_backup[0], clean=True, recursive=True)

            # Need better way to deal with ABORTs.  Should really be abstracted
            # by BongoZk.

            abort_path = old_backup[0].replace(server_type.Mongos, 'ABORT')
            bz.delete(abort_path, clean=True)

      logging.info("mongos_controller: done")
      bz.stop()

   except BackupsDisabled:
      raise
   except BongoZk.BongoZkServerTypeExists:
      db.disconnect()
      logging.info("apparently another mongos server "
                   "is already handling the backup process - exiting...") 
      raise SystemExit(0) 
   except BongoZk.BongoZkException as e:
      raise BackupAbort(bz,
                        "mongos_controller: aborting with "
                        "ZooKeeper-related error: %s" % e)
   except BongoMongo.BongoMongoException as e:
      raise BackupAbort(bz,
                        "mongos_controller: aborting with "
                        "MongoDB-related error: %s" % e)
   except BackupError as e:
      raise BackupAbort(bz, "mongos_controller: aborting: %s" % e)
   except Exception as e:
      traceback.print_exc(file=sys.stderr)
      raise BackupAbort(bz, "mongos_controller: aborting with uncaught "
                            "exception: %s" % e)
   


def config_server_controller(db, zk_root, zk_servers, env, s3bucket, s3key):
   """
   - get my lock & join the party (done by BongoZk)
   - wait until the balancer is stopped
   - shutdown my DB process
   - dump my config DB
   - move dump file to S3
   - put S3 info in ZK (notifies that we're done w/ backup)
   - clean up my local artifacts
   - wait until all the shards are backed up.
   - restart my DB process
   - leave the party and unlock (done by BongoZk)
   """

   def my_abort(msg=None):
      logging.critical("config_server_controller: abort event detected: %s" % msg)

      if db.stopped:
         logging.critical("config_server_controller: attempting db startup")
         db.startup()

      logging.critical("config_server_controller: exiting...")
      os._exit(1)

   try:
      bz = BongoZk(zk_root,
                   server_type.Config,
                   servers = zk_servers, 
                   hour=True)

      if not bz.is_enabled():
         raise BackupsDisabled

      bz.watch4abort(my_abort)

      aws = BongoAws()

      # Record our ID and region info
      me = {'server': bz.id, 'region' : aws.region}
      bz.set_znode4today(str(me))

      logging.info("config_server_controller: waiting for balancer to be stopped")
      try:
	 bz.wait4event(event=event.BalancerStopped, timeout=1800)
      except BongoZk.BongoZkEventTimeout as e:
	 raise BackupAbort(bz,
	       "config_server_controller: %s: timed out waiting for balancer stop" % bz.id)

      logging.info("config_server_controller: shutting down my mongod")
      db.shutdown()

      bz.notify(event.ConfigServerShutdown)

      # dump config db 
      dumpfile = "/tmp/mongodb_configdb_backup_%s-%s.tgz" \
                 % (bz.datestamp, bz.timestamp)
      db.dump('config', dumpfile)

      # Store config db backup in S3
      key = "%s/%s/config_server/%s-%s" % (s3key, env, bz.datestamp, bz.timestamp)
      logging.info("config_server_controller: storing backup on S3: "
                   "bucket = %s, key = %s" % (s3bucket, key))
      aws.write_file_to_s3(s3bucket, key, dumpfile)

      # Put S3 info in ZK (notifies shard servers that config is backed up
      value = str({'bucket':s3bucket, 'key':key})
      bz.notify(event.ConfigServerBackup, value)

      # Clean up dumpfile
      try:
         os.unlink(dumpfile)
      except os.error as e:
	 logging.warning("Error removing local config backup: %s: %s" % (dumpfile, e))

      logging.info("config_server_controller: completed my backup")
      logging.info("config_server_controller: waiting for all backups to complete")
      try:
	 bz.wait4event(event=event.BackupsComplete, timeout=1800)
      except BongoZk.BongoZkEventTimeout as e:
	 raise BackupAbort(bz,
	       "config_server_controller: %s: timed out waiting for all backups to complete" % bz.id)

      logging.info("config_server_controller: starting my mongod")
      db.startup()

      bz.notify(event.ConfigServerStarted)

      clean_days = int(bz.config_get(configKeys.Clean))

      if clean_days:

         from ast import literal_eval

         logging.info("config_server_controller: cleaning configdb backups older than %d days..." % clean_days)

         for old_backup in bz.archive(clean_days, event.ConfigServerBackup):

            old_s3_info = literal_eval(old_backup[1])
            s3_bucket = old_s3_info['bucket']
            s3_key = old_s3_info['key']

            aws.delete_from_s3(s3_bucket, s3_key)

            old_config_path = old_backup[0].replace(event.ConfigServerBackup,
                                                    server_type.Config)
            bz.delete(old_config_path, clean=True, recursive=True)

      logging.info("config_server_controller: done")
      bz.stop()

   except BackupsDisabled:
      raise
   except BongoZk.BongoZkServerTypeExists:
      db.disconnect()
      logging.info("apparently another config server "
                   "is already handling the backup process - exiting...") 
      raise SystemExit(0) 
   except BongoZk.BongoZkException as e:
      raise BackupAbort(bz,
                        "config_server_controller: aborting with "
                        "ZooKeeper-related error: %s" % e)
   except BongoMongo.BongoMongoException as e:
      raise BackupAbort(bz,
                        "config_server_controller: aborting with "
                        "MongoDB-related error: %s" % e)
   except BongoAws.BongoAwsException as e:
      raise BackupAbort(bz,
                        "config_server_controller: aborting with "
                        "AWS-related error: %s" % e)
   except BackupError as e:
      raise BackupAbort(bz, "config_server_controller: aborting: %s" % e)
   except Exception as e:
      traceback.print_exc(file=sys.stderr)
      raise BackupAbort(bz, "config_server_controller: aborting with uncaught "
                            "exception: %s" % e)


def shard_server_controller(db, zk_root, zk_servers, data, env):
   """
   - get my replica-specific lock and join the party (what rep set am I?)
     (done by BongoZk)
   - wait until the balancer is stopped
   - wait until the config server is backed up
   - freeze status as secondary (let's say for 15 minutes)
   - lock my db
   - snapshot my raid vols
   - unlock my db
   - store list of snaps in ZK
   - leave the party and unlock (done by BongoZk)
   """

   def my_abort(msg=None):
      logging.critical("shard_server_controller: abort event detected: %s" % msg)

      if db.is_locked():
         logging.critical("shard_server_controller: attempting DB unlock")
         db.unlock()

      logging.critical("shard_server_controller: exiting...")
      os._exit(1)

   try:

      aws = BongoAws()

      replset_name = db.replica_set_name()

      if not replset_name:
         raise Exception("cannot determine replica set name")

      my_desc = server_type.Shard + "/" + replset_name + "/" + aws.region
      snap_desc = "%s - %s" % (env, replset_name)
      snap_name = "DB Backup - %s" % replset_name

      bz = BongoZk(zk_root,
                   my_desc,
                   servers = zk_servers, 
                   hour=True)

      if not bz.is_enabled():
         raise BackupsDisabled

      bz.watch4abort(my_abort)

      logging.info("shard_server_controller: waiting for balancer to be stopped")
      try:
	 bz.wait4event(event=event.BalancerStopped, timeout=1800)
      except BongoZk.BongoZkEventTimeout as e:
	 raise BackupAbort(bz,
	       "shard_server_controller: %s: timed out waiting for balancer stop" % bz.id)

      # Do we really need to wait for the config backup to finish?

      logging.info("shard_server_controller: waiting for config db backup")
      try:
	 bz.wait4event(event=event.ConfigServerBackup, timeout=600)
      except BongoZk.BongoZkEventTimeout as e:
	 raise BackupAbort(bz,
	       "shard_server_controller: %s: timed out waiting for config backup" % bz.id)

      # Sleep a random amount of time between 1 and 180 seconds to delay simultaneous AWS API calls
      # across all the shard servers
      sleep(randint(1,180))

      db.refuse_nomination(15*60)

      logging.info("shard_server_controller: locking my db against writes")
      db.lock()

      logging.info("shard_server_controller: creating snapshots of my raid vols")
      snapshots = snapshot_ebs_raid(aws, data, snap_name, snap_desc, env, bz.id)

      logging.info("shard_server_controller: unlocking my db")
      db.unlock()

      # Record our ID and snapshot info
      me = [ {
            'server': bz.id,
            'region' : aws.region,
            'snapshots' : snapshots
            } ]
      bz.set_znode4today(str(me))

      clean_days = int(bz.config_get(configKeys.Clean))

      if clean_days:

         from ast import literal_eval

         logging.info("shard_server_controller: cleaning snapshots older than %d days..." % clean_days)

         for old_backup in bz.archive(clean_days, my_desc):
            old_index_path = old_backup[0]
            old_index_data = literal_eval(old_backup[1])
            if 'region' in old_index_data[0] and old_index_data[0]['region'] == aws.region:
               old_snaps = old_index_data.pop(0)['snapshots']

               for snap_id in old_snaps:
                  aws.delete_snapshot(snap_id)

               if len(old_index_data) == 0:
                  bz.delete(old_index_path, clean=True)
               else:
                  bz.zk.set(old_index_path, old_index_data)

	    # Backwards compatibility with how we used to store snapshot info
	    else:
	       old_snaps = literal_eval(old_backup[1])
	       for snap_id in old_snaps:
		  aws.delete_snapshot(snap_id)

               if len(old_index_data) == 0:
                  bz.delete(old_index_path, clean=True)
               else:
                  bz.zk.set(old_index_path, old_index_data)

      logging.info("shard_server_controller: done")
      bz.stop()

   except BackupsDisabled:
      raise
   except BongoZk.BongoZkServerTypeExists:
      db.disconnect()
      logging.info("apparently another shard server for %s "
                   "is already handling the backup process - exiting..." 
                   % replset_name) 
      raise SystemExit(0) 
   except BongoZk.BongoZkException as e:
      db.unlock()
      raise BackupAbort(bz,
                        "shard_server_controller: aborting with "
                        "ZooKeeper-related error: %s" % e)
   except BongoMongo.BongoMongoException as e:
      db.unlock()
      raise BackupAbort(bz,
                        "shard_server_controller: aborting with "
                        "MongoDB-related error: %s" % e)
   except BongoAws.BongoAwsException as e:
      db.unlock()
      raise BackupAbort(bz,
                        "shard_server_controller: aborting with "
                        "AWS-related error: %s" % e)
   except BackupError as e:
      db.unlock()
      raise BackupAbort(bz, "shard_server_controller: aborting: %s" % e)
   except Exception as e:
      db.unlock()
      traceback.print_exc(file=sys.stderr)
      raise BackupAbort(bz, "shard_server_controller: aborting with uncaught "
                            "exception: %s" % e)


# Deprecated
def snapshot_copy(db, zk_root, zk_servers):
   """
   - get my region/replset lock 
     (done by BongoZk)
   - wait for the backups to finish
   - pull list of snaps from ZK
   - request copy of each snap from source region
   - release my lock (done by BongoZk)
   """

   def my_abort(msg=None):
      logging.critical("snapshot_copy: abort event detected: %s" % msg)

      logging.critical("snapshot_copy: exiting...")
      os._exit(1)

   try:

      from ast import literal_eval

      replset_name = db.replica_set_name()

      if not replset_name:
         raise Exception("cannot determine replica set name")

      aws = BongoAws()
      
      shard_key = server_type.Shard + "/" + replset_name
      my_desc = shard_key + "/copies/" + aws.region
      snap_desc = "%s - %s" % (env, replset_name)

      bz = BongoZk(zk_root,
                   my_desc,
                   servers = zk_servers, 
                   hour=True)

      if not bz.is_enabled():
         raise BackupsDisabled

      bz.watch4abort(my_abort)

      logging.info("snapshot_copy: waiting for backups to complete")
      bz.wait4event(event.BackupsComplete)

      shardBz = BongoZk(zk_root,
                        shard_key,
                        servers = zk_servers,
                        hour=True,
                        waitForLock=True)

      backup_info = literal_eval(shardBz.get_znode4today())

      source_region = backup_info[0]['region']

      if source_region == aws.region:

         # This should probably never happen but it's not necessarily a
         # big deal if it does.  It just means that we don't need to copy
         # any snapshots since the we're running in the region where the
         # orig snapshots were made.  We'll just log what we found and
         # exit cleanly.

         logging.info("snapshot_copy: snapshots already exists for this region"
                      " - exiting...")
         shardBz.stop()
         bz.stop()
         raise SystemExit(0) 

      snaps = backup_info[0]['snapshots']
      copies = []
      for snap_id in snaps:
         copy_id = aws.copy_snapshot(source_region, snap_id, snap_desc);
         copies.append(copy_id)

      me = {
            'server': bz.id,
            'region' : aws.region,
            'snapshots' : copies
           }

      backup_info.append(me)

      # Should have exclusive access here since we used waitForLock=True above
      shardBz.set_znode4today(backup_info, update=True)
      
      clean_days = int(bz.config_get(configKeys.Clean))

      if clean_days:

         logging.info("snapshot_copy: cleaning snapshots...")

         for old_backup in bz.archive(clean_days, my_desc):
            old_index_path = old_backup[0]
            old_index_data = literal_eval(old_backup[1])
            for record in old_index_data[:]:  # slice creates list copy (neato)
               if record['region'] == aws.region:
                  old_snaps = record['snapshots']
                  old_index_data.remove(record)
                  
                  for snap_id in old_snaps:
                     aws.delete_snapshot(snap_id)

                  if len(old_index_data) == 0:
                     bz.delete(old_index_path, clean=True)
                  else:
                     bz.zk.set(old_index_path, old_index_data)

      logging.info("snapshot_copy: done")
      shardBz.stop()
      bz.stop()

   except BackupsDisabled:
      raise
   except BongoZk.BongoZkServerTypeExists:
      db.disconnect()
      logging.info("apparently another shard server for %s in %s "
                   "is already handling the snapshot copy - exiting..." 
                   % (replset_name, aws.region)) 
      raise SystemExit(0) 
   except BongoZk.BongoZkException as e:
      raise BackupAbort(bz,
                        "snapshot_copy: aborting with "
                        "ZooKeeper-related error: %s" % e)
   except BongoMongo.BongoMongoException as e:
      raise BackupAbort(bz,
                        "snapshot_copy: aborting with "
                        "MongoDB-related error: %s" % e)
   except BongoAws.BongoAwsException as e:
      raise BackupAbort(bz,
                        "snapshot_copy: aborting with "
                        "AWS-related error: %s" % e)
   except BackupError as e:
      raise BackupAbort(bz, "snapshot_copy: aborting: %s" % e)
   except Exception as e:
      traceback.print_exc(file=sys.stderr)
      raise BackupAbort(bz, "snapshot_copy: aborting with uncaught "
                            "exception: %s" % e)


def main():

   args = parse_args()

   logging.root.name = 'bongo'
   logging.root.setLevel(logging.INFO)

   formatter = logging.Formatter('%(name)s: %(levelname)s: %(message)s')

   syslog = SysLogHandler(address='/dev/log')
   syslog.setFormatter(formatter)
   syslog.setLevel(logging.INFO)
   syslog.addFilter(BackupLogFilter('bongo'))
   logging.root.addHandler(syslog)

   if args.verbose:
      stderr = logging.StreamHandler()
      stderr.setFormatter(formatter)
      stderr.setLevel(logging.INFO)
      logging.root.addHandler(stderr)

   if not args.solo:
      try:
         yfile = yaml.safe_load(open(args.zkconfig))
         zk_servers = ','.join("%s:%s" % (s['host'],s['port'])
                             for s in yfile['zookeepers'])
      except Exception as e:
         logging.critical("Error reading ZooKeeper servers from yaml: %s: %s"
                          % (args.zkconfig, e))
         raise SystemExit(1)

      zk_root = "%s/%s" % (args.zkroot.rstrip('/'), args.env)

   try:

      if args.disable:
         disable_backups(zk_root, zk_servers, args.disable)
         raise SystemExit(0)

      if args.enable:
         enable_backups(zk_root, zk_servers)
         raise SystemExit(0)

      if args.clean != None:
	 config_set(zk_root, zk_servers, configKeys.Clean, args.clean)
         raise SystemExit(0)


      if args.unlock:
	 db = BongoMongo(ignore_version=True)
         if not db.is_mongos and not db.is_config:
	    unlock_controller(db)
         sys.exit(0)

      else:
	 db = BongoMongo()

      if args.solo:
         if db.is_mongos or db.is_config:
            raise SystemExit("Solo mode can only be used on a shard server - "
                             "exiting...")
         solo_controller(db, args.data, args.env)
         sys.exit(0)

      if db.is_mongos:
	 logging.info("I think I'm a Mongos server")
	 mongos_controller(db, zk_root, zk_servers)
      elif db.is_config:
	 logging.info("I think I'm a Config server")
	 config_server_controller(db, zk_root, zk_servers,
                                  args.env, args.s3bucket, args.s3key)
      elif not db.is_primary:
	 logging.info("I think I'm a Shard server")

         # continue shard backup procedure if replication is up-to-date
	 if db.is_fresh():
	    shard_server_controller(db, zk_root, zk_servers, args.data, args.env)
	 else:
	    logging.info("I'm over 30 minutes behind on replication.  We'll let another secondary do the backup - exiting...")
	    sys.exit(0)

      else:

	 logging.info("not a mongos or config or non-primary shard server - "
                      "exiting...")

      sys.exit(0)

   except BackupsDisabled:
      logging.warning("Backups are currently disabled.  Exiting...")
      db.disconnect()
      raise SystemExit(0)
   except Exception as e:
      logging.critical("Terminating with uncaught exception: %s" % e)
      traceback.print_exc(file=sys.stderr)
      raise SystemExit(1)


if __name__ == '__main__': main()

